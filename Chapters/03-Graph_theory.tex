% ************************************************
\chapter{Graph Theory}\label{ch:Graph_theory} 
% ************************************************
In this chapter we review basic graph theory and explain how these
terms are applicable in the context of biological neural networks. We
begin with the definition of directed graphs: %?? needs rework

\section{Directed graphs}\label{sec:directed_graphs}


%  \begin{definition}[Graph, Order] A \textbf{graph} is a pair of sets $G
% =(V,E)$, consisting of the \textbf{vertex set} $V(G)=V$ and the
% \textbf{edge set} $E(G)=E$, such that $E \subseteq V \times V$.  The
% number of vertices of a graph $G$ is its \textbf{order} $|G|$, the
% number of edges is denoted by $||G||$.  A graph of order $0$ or $1$ is
% called \textit{trivial}.
%   \end{definition}

%   The object of interest in my thesis will be directed pseudographs,
% but I will still have to think about how to
%   \begin{itemize}
%   \item call them, graphs, directed graphs, etc..
%   \item think about the weights(!!)
%   \item think about definitions
%   \end{itemize}


\begin{definition}[Directed graphs]
  \index{directed graph} \index{simple graph} \index{pseudograph}
  \index{multigraph} \index{loop graph}
  \label{def:directed_graphs}
  A \textbf{directed pseudograph} $G$ consists of two finite
  %
  % Non-empty? -> No. Empty graph allowed for random 
  % graph process that adds edges and or vertices
  % 
  $V$, the \textit{set of vertices} of $G$, and $E$, the \textit{set
    of edges} of $G$, and two maps $ s,t: E \to V, $ the
  \textit{source} and \textit{target functions} of $G$. A
  \textbf{directed multigraph} is a directed pseudograph without
  \textit{loops}, that is the map $d = (s,t):E \to V^2$ already maps
  maps to $V^2\setminus\Delta_V$, where $V^2 = V \times V$ denotes the
  cartesian product and $\Delta_V = \{(x,x) \mid x \in V\} \subseteq
  V^2$ the diagonal. Similarily, a \textbf{directed loop graph} is a
  directed pseudograph where $d$ is injective. Finally, a
  \textbf{simple directed graph} can be defined as a directed
  pseudograph where $d$ is both injective and already maps to
  $V^2\setminus\Delta_V$.
\end{definition}

Thus, in simple directed graphs, neither parallel edges nor loops -
edges between the same vertex - are allowed, whereas directed
multigraphs and directed loop graphs admit one of them respectively.

\red{Say something about what "directed graph" means here, do all
  definition until random graphs work for all types of directed graphs?}

Given a directed graph $G$, we denote with $V(G)$ the set of vertices
of $G$ and call it the \textbf{vertex set} of $G$. Analogously, the
\textbf{edge set} $E(G)$ of $G$ denotes the set of edges of $G$. This
means, for a directed graph specified as $G = (V,E,s,t)$, we
have \[V(G) = V \quad \mathrm{and} \quad E(G) = E.\]

\begin{figure}[!htbp]
  \centering 
  \makebox[0.6\textwidth]{%
    \begin{overpic}[width=0.25\textwidth]{%
        tikz/directed_pseudograph.pdf}
      \put(-15,35){\small\textbf{A}}
    \end{overpic}
    \hfill
    \begin{overpic}[width=0.25\textwidth]{%
        tikz/directed_multigraph.pdf}
      \put(3,35){\small\textbf{B}}
    \end{overpic}
    }%
  \vfill
  \vspace{0.25cm}
  \makebox[0.6\textwidth]{%
    \begin{overpic}[width=0.25\textwidth]{%
        tikz/directed_loopgraph.pdf}
      \put(-15,35){\small\textbf{C}}
    \end{overpic}
    \hfill
    \begin{overpic}[width=0.25\textwidth]{%
        tikz/directed_simple_graph.pdf}
      \put(3,35){\small\textbf{D}}
    \end{overpic}
    }%
    \caption{%
      \textbf{Typical examples of the directed graph types}
      \textbf{A)} directed pseudograph \textbf{B)} directed
      multigraph \textbf{C)} directed loop graph \textbf{D)} simple
      directed graph.} %??
  \label{fig:directed_graph_types}
\end{figure}



A \textbf{morphism} $\phi: G \to H$, between two directed graphs
$G=(V_G,E_G,s_G,t_G)$ and $H=(V_H,E_H,s_H,t_H)$, consists of a pair of
maps $\phi_V: V_G \to V_H$ and $\phi_E: E_G \to E_H$, such that
\[
s_H \circ \phi_E = \phi_V \circ s_G \mathrm{\quad and \quad} t_H \circ
\phi_E = \phi_V \circ t_G,
\]
that is such that the following diagram commutes:
%
\begin{align*} 
  \xymatrix@+=1.3cm{E_G \ar^{t_G}@<0.66ex>[d] \ar_{s_G}@<-0.66ex>[d]
    \ar^{\phi_E}[r] & E_H \ar^{t_H}@<0.66ex>[d]
    \ar_{s_H}@<-0.66ex>[d]\\ V_G \ar^{\phi_V}[r] & V_H}
\end{align*}
%
A morphism $\varphi: G \to H$, between two directed pseudographs $G$
and $H$ is an \textbf{isomorphism}, if the maps $\varphi_V: V_G \to
V_H$ and $\varphi_E: E_G \to E_H$ are bijective. Two directed
pseudographs are called \textit{isomorphic} if there exists an
isomorphism inbetween them.

\begin{remark}
  The last definition implies that, if there exists an isomorphism
  $\varphi: G \to H$, an isomorphism $\psi: H \to G$ can be
  found. This isomorphism is, of course, easily constructed via
  $\psi_V: V_H \to V_G, v \mapsto \varphi_V^{-1}(v)$, $\psi_E: E_H \to
  E_G, e \mapsto \varphi_E^{-1}(e)$.
\end{remark}

\begin{definition}[Weighted directed graphs]
  An \textbf{edge-weighted directed graph} is a directed graph $G$ along
  with a mapping $\omega: E(G) \to \mathbb{R}$, called the
  \textit{weight function}. Similarily, a \textbf{vertex-weighted
    directed graph} is a directed graph with a mapping $\nu: V(G) \to
  \mathbb{R}$.
\end{definition}

\begin{remark} \red{(heavy draft)}

  % 
  % Think about
  %  - synaptic integration (non-linear)
  %  - self excitation, interessed in axonal-dendritic connections, so
  %    probably not for structure but maybe dynamics?
  % 

  % Title: A directed graph category for biological neural networks

  Certainly, a weighted directed pseudograph is the most fitting
  mathematical modelling to the biological situation, since self
  connections and multiple synapse are not only plausibel (source?)
  but the rule. However there is one abstraction we can make by adding
  together the synaptic weights - NEST is doing it. While linear
  synaptic integration was shown to be, it is a prevalent model in
  network (ref Nest).\\ 

  Also think about \textbf{inhibitory, excitatory}. Suggestion: edge
  weights $\omega: E(G) \to \mathbb{R}^{+}$ \textbf{and} vertex
  weights $\nu: V(G) \to \{-1, 1\}$. Synaptic weight $\mathrm{syn}(e)$
  for edge $e$ is then \[\mathrm{syn}(e) = \nu(s(e))\,\omega(e).\]
  Benefit: Synapse from one neuron are either excitatory or inhibitory
  but not mixed as in bio.
\end{remark}


\begin{remark}[Equivalent definiton for directed loop graphs]

  A directed loop graph $G$ can be equivalently defined as a pair of
  finite\red{(, non-empty?)} sets $V$, the \textit{set of vertices} of
  $G$, and $E \subseteq V^2$ the \textit{set of edges} of $G$. For an
  edge $(x,y) \in E$, we call $x$ the \textit{source} and $y$ the
  target of the edge $(x,y)$.

  Source and target functions are then uniquely determined as the
  projections on the first and second component, $s = \mathrm{pr}_1, t
  = \mathrm{pr}_2: E(G) \to V.$ Conversely, the edge set $E(G)
  \subseteq V^2$ can be determined from the source and target
  functions as $E:=\{(s(e),t(e)) \mid e \in E\}$. The trivial identities
  $(x,y) = (\mathrm{pr}_1(x,y),\mathrm{pr}_2(x,y))$ and
  $\mathrm{pr_1}(s(e), t(e)) = s(e)$ with $\mathrm{pr_2}(s(e), t(e)) =
  t(e)$ quickly verify the equivalence of the definitions.

  Given a directed loop graph $G$, we often assume the graph to be
  given in this form and write edges as $e=(x,y)$. Note that this
  concept is more complicated to introduce for directed pseudographs,
  since parallel edges $e$ and $e'$ should to be differentiated in the
  egde set of $G$, establishing the need for $E(G)$ to be a multi- or
  indexed set, notions we are trying to avoid in this document.

\end{remark}



% A given directed loop graph $G = (V,E,s,t)$, can always be
% represented by a canonical isomorphic directed loop graph $G' =
% (V,E')$, where $E':=\{(s(e),t(e)) | e \in E\} \subseteq V^2$. For a
% directed loop graph $G'$ in canonical form, the source and target
% functions $s',t'$ do not need to be specified, since they are uniquely
% determined as the projections on the first and second component, $s' =
% \mathrm{pr}_1, t' = \mathrm{pr}_2$. Wait, not so easy because
% multi-sets, however add synaptic strength of parallel edges to make
% directed loop-graphs! (Bang-Jensen)



From now on any \textit{directed graph} is assumed to be a directed
loop graph. Although most, if not all, concepts work for directed
pseudographs just as well, we want to start to heavily use the
canonical edge representation, which when talking about pseudograps
makes problems as mentioned before.

\begin{remark}[More Notation] 
  \red{- Check, do I really need this?} For a pair of vertex sets $X,Y
  \subseteq V(G)$ of a directed graph $G$ we write
  \[
  (X,Y)_G = \{(x,y) \in E(G) | x \in X, y \in Y \}
  \]
  for the set of edges with source in $X$ and target in $Y$. For
  vertex sets with a single element $X = {x}$, we also write $(x,Y)_G$
  and mean the edges with source $x$ and target in $Y$.
\end{remark}


\begin{definition}[In- and out-degree]  
  \index{in-degree}\index{out-degree}
  For a directed graph $G$ the \textbf{in-degree} $d^-_G(x)$ of a
  vertex $x$ is defined as the number of edges of $G$ with target $x$,
  that is
  \[
  d^-_G(x) = \left|(V(G),x)_G\right|.
  \]
  Similarily, the \textbf{out-degree} $d^+_G(x)$ of $x$ is defined as
  \[
  d^+_G(x) = \left|(x, V(G))_G\right|,
  \]
  the number of edges in $G$ with source $x$.
\end{definition}

\begin{remark}[Side]
  In some literature about directed graphs (Bang-Jensen), loops are
  \textit{not} counting towards the in- or out-degree of vertex. In
  the light of neural network however, we specifically want to count
  loops as well.
\end{remark}

A basic property of the in- and out-degree in directed graphs is that
number of in-degrees of every vertex, as well the sum of every
out-degree, equal the total number of edges:

\begin{proposition}
  In every directed graph $G$, we have
  \[
  \sum_{x \in V(G)} d^-(x) = \sum_{x \in V(G)} d^+(x) = | E(G) |.
  \]
\end{proposition}

\begin{proof}
  Since $(V(G),x)_G \cap (V(G),y)_G = \emptyset$ for $x \ne y$, we can
  write
  \[
  \sum_{x \in V(G)} d^-(x) = \left| \bigcup_{x \in V(G)} (V(G),x)_G
  \right| = \left| (V(G),V(G))_G \right| = | E(G) |.
  \]
  Analogously for the out-degree.
\end{proof}


\section{Walks and distances}

Let $G$ be a directed graph \red{(what does it mean here?)}. A
\textbf{walk} $W$ in $G$ is an alternating sequence
$(x_1,e_1,x_2,e_2,x_3,\ldots,x_{n-1},e_{n-1},x_n)$ of of vertices
$x_i$ and edges $e_i$ from $G$, such that
\[
s(e_i) = x_i \quad \mathrm{and} \quad t(e_i) = x_{i+1}, \:\,
\mathrm{for}\, i=1,..,n-1,
\]
that is, such that the vertices are connected by the edges inbetween
them. We denote the set of vertices $(x_1,\ldots,x_n)$ of $W$ as
$V(W)$ and the sequence of edges $(e_1,\dots,e_{n-1})$ as $E(W)$
\red{(need it?)}.

The vertices $x_1$ and $x_n$ are called the \textit{end vertices} of
$W$ and we also say that $W$ is an $(x,y)$-walk. The \textbf{length}
of $W$ is defined as the length of the sequence of edges; a walk
consisting of only one vertex has length zero. \red{ colon, really?}


\begin{definition}[Distance]
  The \textbf{distance} of two vertices $x$,$y$ in a directed graph
  $G$ \red{(means?)}, is defined as the minimum length of an
  $(x,y)$-walk, if any such walk exists, otherwise
  $\operatorname{dist}(x,y)=\infty$. In short,
  \[
  \operatorname{dist}(x,y) = \inf \{|E(W)| \mid
  W\,\mathrm{is}\,(x,y)\mathrm{-walk}\}.
  \]
  \red{$|E(W)|$ is not explained. Necessary?}
\end{definition}

\begin{proposition}
  The distance function $\operatorname{dist}: V(G) \times V(G) \to
  \mathbb{N}$ of a directed graph $G$ satisfies the triangle equality,
  \[
  \operatorname{dist}(x,z) \le \operatorname{dist}(x,y) +
  \operatorname{dist}(y,z), \:\: \mathrm{for}\:\, x,y,z \in V(G).
  \]
\end{proposition}

\begin{proof}
  Let $x,y,z$ be vertices in $G$. If either no $(x,y)$-walk or
  $(y,z)$-walk exists, the inequality holds by definition. Other wise,
  let $W$ be an $(x,y)$-walk of minimal length and let $U$ be a
  $(y,z)$-walk of minimal length. Certainly, by concatenating $W$ and
  $U$ we obtain an $(x,z)$-walk of length $|E(W)| + |E(U)| =
  \operatorname{dist}(x,y) + \operatorname{dist}(y,z)$, proofing
  that \[ \operatorname{dist}(x,z) \le \operatorname{dist}(x,y) +
  \operatorname{dist}(y,z).
  \]
\end{proof}





%   \begin{definition}[Neighbour, adjacent] Two \textbf{vertices} $x,y \in
% V(G)$ of $G$ are called \textit{adjacent} or \textit{neighbours} if
% there is an edge between $x$ and $y$, $(x,y) \in E(G)$. Two
% \textbf{edges} $e \neq f$ are \textit{adjacent} if they have an end in
% common.
%   \end{definition}


More to do:

\begin{itemize}
\item summarize category of directed (weighted) pseudographs
\item weights!
\item vertices will also be called nodes and neurons, edges will also
  be connections or synapses.
\item subgraphs
\item \sout{vertex set, edge set $E(G), V(G)$.}
\item $\omega(e)$ is weight, connection strength or synaptic weight
  (as a side remark
\item extend to category of weighted directed pseudographs
  (isomorphisms)
\item path
\item adjacency matrix
\item converses of graph related to opposite category?
\item \sout{in- and out-degree}
\item \sout{triangle inequality for distance, $\mathrm{dist}(x,z) \leq
  \mathrm{dist}(x,y) + \mathrm{dist}(y,z)$}
\end{itemize}



References for this chapter:
\url{http://nlab.mathforge.org/nlab/show/graph},
\url{http://nlab.mathforge.org/nlab/show/quiver}, \parencite{Bang-Jensen_Digraphs} 




\section{Random Graph Theory}\label{sec:random_graph_theory}
\index{random graph}%?? where

From this chapter on, as it is common and practical when talking about
random graph models, we move away from the the abstract notion
%------------------------------------------------
\marginpar{focus on labeled, simple directed graphs}
%------------------------------------------------ 
of graphs and their equivalence classes and consider \textit{labeled
  graphs}, where the edge set of a graph with $n$ vertices takes the
form $V = \{1,\ldots,n\}$. Simple directed graphs constitute the
fundamental mathematical object underlying the concepts developed in
this work and if not specified otherwise, all graphs are assumed to be
labeled and simple directed.
%?? Focus on simple directed graphs!

The concept of a random graph was first formally introduced by
\textcite{Erdos1959}. In their $G(n,N)$ model, a graph with $n$
vertices and $N$ edges is randomly and with equal probability selected
from the set of graphs with this configuration. In the same year
\textcite{Gilbert1959} independently introduced his closely related
$G(n,p)$ model, which we define in detail in
\ref{def:gilbert_random_graph}.

A random graph model is a probability space over a set of specific graphs.  Erd\H{o}s et al. and Gilbert describe their
. Often implicit. 

The term \enquote{random graph} can then refer to a random graph model
or one sample in the probability. Here we try to avoid this ambiguity
and refer. The object of 

Keeping in mind that the term \enquote{graph} now refers to labeled,
simple directed graphs if not otherwise specified we denote with $G^n$
the set of directed graphs with $n$ vertices, 
\[
G^n := \{G \mid G\,\mathrm{\,graph},\, |V(G)| = n\}.
\]
The $G(n,p)$ model was first introduced by \textcite{Gilbert1959}
closely related to Erd\H{o}s and R\'{e}nyi's $G(n,m)$
model. \textcite{Bollobas_Random-graphs}

\begin{definition}[Gilbert random graph model]
  \label{def:gilbert_random_graph} \index{Gilbert random graph model}
  Let $n\in\mathbb{N}$ and $0\leq p \leq 1$. The \textit{Gilbert
    random graph model} $G(n,p)$ is a discrete probability space over $G^n$
  with a probability measure $P$, such that every graph $G$ with $\abs{E(G)}=k$ edges
  appears with equal probability%
  \[%
    P(G) = {p^k(1-p)^{n(n-1)-k}},%
  \]%
  for $0 \leq k \leq n(n-1)$. 
\end{definition}

\begin{remark}Clearly $G(n,p)$ is well-defined, as there exist $\binom{n(n-1)}{k}$
distinct labeled graphs with $n$ vertices and $k$ edges and thus 
\[
  \sum_{G \in G^n} P(G) =  \sum_{k=0}^{n(n-1)}  \binom{n(n-1)}{k}
  p^k(1-p)^{n(n-1)-k} = 1, % = (p+(1-p))^{n(n-1)}
\]
after the binomial theorem.
% Which sigma algebra -> power set (discrete probability space)
\end{remark}


Equivalently, the Gilbert random graph model can be defined as a
stochastic process;
%------------------------------------------------
\marginpar{equivalent definition as random process} 
%------------------------------------------------ 
to an empty graph with $n$ vertices, for each of the $n(n-1)$ vertex
pairs an edge is added at random and independently with probability
$p$. The probability to obtain a specific graph $G$ with $k$ edges is
then obviously $p^k(1-p)^{n(n-1)-k}$, already proofing the
equivalence, since assuming a process as above with edge probability
$p'$ such that the induced probability measure on $G^n$ equals
$P$ from \ref{def:gilbert_random_graph}, already yields $p = p'$ in
the choice of $n=2$ and $k=1$. 


\begin{proposition}
  In- and out-degree distribution of vertices in the Gilbert random
  graph model are Poisson for large $n$.
\end{proposition}
%
\begin{proof}
  Let $X$ be a random variable on the random graph model, mapping to
  the in- or out-degree $d_G(v)$ of a vertex $v$ of a graph $G \in
  G^n$. There are $n-1$ other vertices that, with probability $p$,
  project to $v$ (receive input from $v$), thus
  \[
    P(X=k) = \binom{n-1}{k} p^k (1-p)^{n-1-k}.
  \]
  The binomial distribution i $P(k) = \mathcal{B}_{n-1,p}(k) \approx $.

 % $X: G^n \to \mathbb{R}, G \to |E(G)|$, discrete random variable,
 %  with probability distribution $\operatorname{B}_{n(n-1),p}$.
 %  \[
 %    \operatorname{B}_{n(n-1),p}(k) = \binom{n(n-1)}{k} p^k(1-p)^{n(n-1)-k}
 %  \]
\end{proof}


References: Newman, Erdos1960, Erdos1959, Gilbert1959, Bollabos
Wikipedia, \parencite{West_Graph-theory}
		
		

% \begin{remark}[Expected number of edges in a directed Erd\H{o}s-R\'{e}nyi graph (DERG)]
%   $X: G^n \to \mathbb{R}, G \to |E(G)|$, discrete random variable,
%   with probability distribution $G(n,p) =\operatorname{B}(n^2,p)$,
%   binomial distribution. That is, distribution of $X$ via probability
%   mass function $P(X=k) = {{n^2} \choose k} p^k\,(1-p)^{n^2-k}$. Thus
%   $\operatorname{E}(X)$ equals expect edges in DERG. We have of
%   course, since $G(n,p)$ binomial,
%   \[
%   \operatorname{E}(X) = pn^2. \mathrm{\:(if\,self-edges!)}
%   \]
%   Then, of course, the \textbf{mean in-}  and \textbf{out-degree} is \[ \langle
%   d^{\mathrm{in}} \rangle = \langle d^{\mathrm{out}} \rangle =
%   \frac{\langle |E(G)| \rangle}{|V(G)|} = np.\] \textit{Does this make
%     sense? Define everything properly!!}
% \end{remark}




\section{Geometric Graphs}\label{sec:geometric_graphs} 

Geometric are of embedding t. Planar deal with
\textcite{Diestel_Graph-theory}. Here we relevant for us is a notion
\textcite{Penrose_Geometric-graph}

\begin{definition}[Geometric directed graph]\index{geometric graph} A \textit{geometric
    directed graph} $G_{\Phi}$ is a directed graph $G$ paired with a map
\[
  \Phi:V(G) \to [0,1]^2,
\]
representing vertex positions on the unit square.
\end{definition}

\begin{remark} This definition diverges from the usual notion of a
  geometric graph, which determines the existence of edges only
  between nodes within a spatial distance $x$ in a specified norm
  (usually $l_w$) \parencite[108]{Mesbahi_Multiagent-networks}  Moreover, geometric graphs are usually only
  discussed in the context of random graph
  models \parencite{Gilbert1961}.  
  We denote the set of geometric graphs with $n$ edges by $G_{\Phi}^n$.
  While the concept of spatially
  embedded graph allows to define a random graph model that is highly
  relevant for this study:
\end{remark}

\begin{definition}[Distance-dependent random geometric graph model]
  %\index{distance-dependent random graph model}
  Let $n \in \mathbb{N}$ and $P: [0,\sqrt{2}] \to [0,1]$ a
  continuous %?? C0 analytic?
  map.  A \textit{distance-dependent geometric random graph model}
  $G_{\Phi}(n,P)$ is a random graph model over $G^n_{\Phi}$, generated
  by distributing uniformly at random the $n$ vertices on the unit
  square and adding edges for each vertex pair $(v,w) \in
  {V(G_{\Phi})}^2 \setminus \Delta_{V(G_{\Phi})}$ with a
  probability $P(x)$, depending on the distance $x = \norm{\Phi(v) -
    \Phi(w)}$ between the vertices.
\end{definition}

\begin{proposition}
Expected edges
\end{proposition}
%
\begin{proof}
proofing this
\end{proof}


Need to introduce Position function to use for anisotropic networks
\ref{def:anisotropic_geometric_graph}.




\section{Connectivity Matrix}

\begin{definition}[Connectivity matrix]
% NEED LABELED GRAPH!!
Let $G$ be a directed graph with $n$ vertices. Then the
\textit{connectivity matrix} $C(G)$ of $G$, is such that $C(G)_{ij} =
1$ if there exists an edge $e \in E(G)$ with 
\end{definition}



% From Graph clustering - Satu Elisa Schaeffer - 2007
% 
% [84] P. Erdos, A. Rényi, On random graphs I, in: Selected Papers
% of Alfréd Rényi, vol. 2, Akadémiai Kiadó, Budapest, Hungary,
% 1976, pp. 308–315. First publication in Publ. Math. Debrecen
% 1959.
% [85] P. Erdos, A. Rényi, On the evolution of random graphs,
% in: Selected Papers of Alfréd Rényi, vol. 2, Akadémiai Kiadó,
% Budapest, Hungary, 1976, pp. 482–525. First publication in
% MTA Mat. Kut. Int. Közl. 1960.





%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../dplths_document"
%%% End: 
